{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from transformers import pipeline\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - NER using open soure (hugging face) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lire un fichier docx et extraire son texte.\n",
    "def extract_text_from_docx(docx_path):\n",
    "    data  = Docx2txtLoader(docx_path).load()\n",
    "    return data[0].page_content.strip()\n",
    "\n",
    "#Génèrer les regex à des noms d'entités qu'on souhaite extraire\n",
    "def format_entity_patterns(entity_names):\n",
    "    return [rf\"{re.escape(name)}\\s*\\n(.+)\" for name in entity_names]\n",
    "\n",
    "\n",
    "#Appliquer les regex pour extraire des entités nommées\n",
    "def extract_entities(text,\n",
    "                      entities_names =  [\"Counterparty\", \"Initial Valuation Date\", \"Notional\", \"Valuation Date\", \"Maturity\", \"Underlying\", \"Coupon\", \"Barrier\", \"Calendar\"], \n",
    "                      entities_to_extract = [\"Party A\", \"Initial Valuation Date\", \"Notional Amount (N)\", \"Valuation Date\", \"Termination Date\",  \"Underlying\", \"Coupon (C)\", \"Barrier (B)\", \"Business Day\"]):\n",
    "    entities = {}\n",
    "    patterns = dict(zip(entities_names,  format_entity_patterns(entities_to_extract)))\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text)\n",
    "        entities[key] = match.group(1) if match else None\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Counterparty': 'BANK ABC',\n",
       " 'Initial Valuation Date': '31 January 2025',\n",
       " 'Notional': 'EUR 1 million',\n",
       " 'Valuation Date': '31 January 2025',\n",
       " 'Maturity': '07 August 2026',\n",
       " 'Underlying': 'Allianz SE (ISIN DE0008404005, Reuters: ALVG.DE)',\n",
       " 'Coupon': '0%',\n",
       " 'Barrier': '75.00% of Shareini  ',\n",
       " 'Calendar': 'TARGET '}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docx_text = extract_text_from_docx(\"data/ZF4894_ALV_07Aug2026_physical.docx\")\n",
    "\n",
    "extract_entities(docx_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - NER using open soure (hugging face) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()\n",
    "    \n",
    "def model_loader(model_path) :\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "    return pipeline(\"ner\", model=model, tokenizer=tokenizer,  aggregation_strategy=\"first\")\n",
    "\n",
    "\n",
    "def extract_named_entities_from_text(ner_pipeline, text) :\n",
    "    results = ner_pipeline(text)\n",
    "    entities = [{k: d[k] for k in [\"entity_group\", \"score\", \"word\"]} for d in results]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./pretrained-models/distilbert-NER\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m ner_pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_loader\u001b[49m(model_path)\n\u001b[0;32m      4\u001b[0m text \u001b[38;5;241m=\u001b[39m read_text_file(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/FR001400QV82_AVMAFC_30Jun2028.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m extract_named_entities_from_text(ner_pipeline, text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_loader' is not defined"
     ]
    }
   ],
   "source": [
    "model_path = \"./pretrained-models/distilbert-NER\"\n",
    "ner_pipeline = model_loader(model_path)\n",
    "\n",
    "text = read_text_file(\"data/FR001400QV82_AVMAFC_30Jun2028.txt\")\n",
    "extract_named_entities_from_text(ner_pipeline, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER for Verbose and complex PDF using RAG approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ollama'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_and_rag_ner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m extract_named_entities\n",
      "File \u001b[1;32mc:\\Users\\habou\\Desktop\\python\\GenAI_LLM\\ca-cib-ai-test\\src\\llm_and_rag_ner.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mollama\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_named_entities\u001b[39m(model, text):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ollama'"
     ]
    }
   ],
   "source": [
    "from src.llm_and_rag_ner import extract_named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ollama'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mollama\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_named_entities\u001b[39m(model, text):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ollama'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model='llama3.2'\n",
    "text = \"Elon Musk a fondé SpaceX à Los Angeles. Apple et Google investissent en France.\"\n",
    "\n",
    "# Extraction des entités\n",
    "entities = extract_named_entities(model, text)\n",
    "\n",
    "# Affichage des entités extraites\n",
    "print(json.dumps(entities, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ollama'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mollama\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ollama'"
     ]
    }
   ],
   "source": [
    "import ollama"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
